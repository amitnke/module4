makeVector(1:1000)
makeVector(c(1:1000))
source('~/maleVector.R')
x <- 1:10000
makeVector(x)
vec <- makeVector(x)
vec$getMean()
vec <- makeVector(x)
vec
vec$getmean()
mx <- mean(x)
vec$setmean(mx)
vec$getmean()
vec
source('~/GitHub/ProgrammingAssignment2/cachematrix.R')
x = rbind(c(1, -1/4), c(-1/4, 1))
m = makeCacheMatrix(x)
m$get()
cacheSolve(m)
cacheSolve(m)
make check
make check-all
getwd()
x <- 1
print(x)
x
a <- c(1,2,3,4,5)
b <- c(1:5)
a=b
print(a=b)
msg <- "hello"
msg
a <- b
a
b
a-b
a+b
a=b-a
a+b-a
y <- c(T,2)
class(Y)
class(y)
y <- c("a", F)
class(y)
x <- c(1,2,3,4,5)
class(x)
y <- as.logical(x)
y
x <- c(0,1,2,3,4,5)
y <- as.logical(x)
y
class(y)
z <- as.character(x)
z
class(x)
class(z)
x <- c("a","b")
as.numeric(x)
as.logical(x)
subject_name <- c("john_doe", "jane dae", "Steve Graves")
temprature <- c(98.1, 98.6, 101.4)
flu_status <- c(FALSE, FALSE, TRUE)
temprature[2]
temprature[2:3]
temprature[-2]
temprature[flu_status]
rattle()
data(iris)
str(iris)
iris[2]
str(iris)
iris[2,]
ls()
databases()
plot(iris)
library(A3)
install.packages("Rcmdr")
library("Rcmdr", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
detach("package:Rcmdr", unload=TRUE)
x<-c(3,4,5,8)
y<-c(1,3)
z<-x+y
z
a<-seq(1:100)
a
a <- 1:100
1
a
a <- seq(1,100,2)
a
e<-rep("X",10)
e
a<-rep(1,6)
a
c<-rep(1:6,2)
c
c<-rep(1:6,1:6)
c
x<-rep(c(4,7,1,5),c(3,2,5,2))
x
a<-c(8,7,9,2)
b<-order(a)
b
a<-c(1,2,3)
b<-c(4,5,6)
c<-data.frame(rbind(a,b))
c
d<- data.frame(c(1:3), c(4:6))
d
cube <- function(x,n){}
x^3
cube <- function(x,n){ x^3 }
cube(3)
x <- 1:10
if(x>5){ x <- 0 }
if( x > 5 ) { x <- 0 }
x
if(x > 5) {
x <- 0
}
f <- function(x) {}
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z <- 10
f(3)
a <- vector("numeric", length = 10)
a
y <- data.frame(a=1, b="a")
y
dput(y)
dput(y, file="y.R")
new.y <- dget("y.R")
new.y
getwd()
con <- url("www.google.com","r")
con <- url("https://www.google.com","r")
con <- url("https://www.facebook.com","r")
con <- url("http://hsdavnarkatiaganj.org","r")
x <- readLines(con)
head(x)
x
f <- function(x, y){ x^2 + y/z}
f(2,3)
ls
environment
search()
make.power <- function(n){
pow <- function(x){
x^n
}
pow
}
cube <- make.power(3)
square <- make.power(2)
cube(3)
x
cube(3)
square(4)
search()
ls()
cube
square
help ls
make.NegLogLikk <- function(data, fixed = c(FALSE, FALSE)){
params <- fixed
function(p){
params[!fixed] <- p
mu <- params[1]
sigma <- params[2]
## Calculate the Normal density
a <- -0.5*length(data)*log(2*pi*sigma^2)
b <- -0.5*sum((data-mu)^2) / (sigma^2)
-(a + b)
}
}
set.seed(1)
mormals <- rnorm(100, 1, 2)
nLL <- make.NegLogLikk(mormals)
nLL
ls(environment(nLL))
optim(c(mu = 0, sigma = 1), nLL)$par
seed(1)
get.seed()
help set.seed()
help set.sead
x <- as.Date("1970-01-01")
x
unclass(x)
x
unclass(as.Date("1970-01-02"))
Sys.time()
class(x)
x <- Sys.time()
x
class(x)
p <- as.POSIXct(x)
p
names(unclass(p))
p <- as.POSIXlt(x)
p
names(unclass(p))
p$wday
p$sec
p$hour
p$min
p$mon
p$yday
p$isdst
p
x
class(x)
unclass(x)
datestring <- c("January 10, 2012 10:40", December 9, 2011 9:10)
datestring <- c("January 10, 2012 10:40", "December 9, 2011 9:10")
datastring
datestring
x <- strptime(datestring, "%B %d, %Y %H:%M")
x
class(x)
p <- as.POSIXlt(x)
p
unclass(p)
x <- Sys.time()
x
class(x)
y <- as.Date("2015-05-13 02:23:32")
y
unclass(x)
unclass(x)
p <- as.POSIXlt(x)
names(unclass(p))
p$sec
x <- Sys.Date()
x
datestring()
datestring
search()
Sys.getenvironment()
ls(environment)
ls(environment(GlobalEnv))
ls(environment("GlobalEnv"))
ls(environment(".GlobalEnv"))
names()
names
function(x)
{}
x
install.packages("dplyr")
library("dplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
"Hello" + "World"
?paste
paste(2,3)
paste(2, 3, sep = "and")
paste(2, 3, sep = " and ")
?collapse
??collapse
arr <- c(10,20,30,40)
class(arr)
arr
arr + c(1,2)
b > 20
b <- arr > 20
arr[b]
---Preprocessing for kNN algorithm
data(iris)
str(iris)
---Which species of flower is based on remaining feature
table(iris$Species)
head(iris)
----Mix up the rows as data seems to be ordered with Species
set.seed(9850)
gp <- runif(150)
iris <- iris[order(gp),]
str(iris)
head(iris,10)
---rescale my numerical vector
summary(iris[,c(1,2,3,4)])
---all the column has different range. All feature to be scaled in similar fashion.
--Normalize, value - min / (max - min)
normalize <- function(x) { return ((x-min(x))/(max(x) - min(x))) }
normalize(c(1,2,3,4))
--Now normalize the sepal length, width, petal length, width
summary(iris_n)
kmeans.wss.k <- function(D, k){
km <- kmeans(D, centers = k, nstart = 5)
return (km$tot.withinss)
}
kmeans.wss.k(tastes, 4)
kmeans.wss.k(tastes, 5)
kmeans.wss.k(tastes, 7)
kmeans.wss.k(tastes, 8)
kmeans.wss.k(tastes, 10)
D <- read.table("whiskies.txt",header = T, sep = ",")
#Data is from Scotland Whisky
class(D)
str(D)
#Distillary information along with Bunch of testing score in range of 0-4.
#Also location information of location of distillary
#Each record is one distiallry information
#what the whisky that have similar taste. Also we will try to find the correlation between
#location and whisky taste
tastes = D[,3:14]
str(tastes)
#Devide the datastet into 4 cluster and 5 starting point
kmfit = kmeans(tastes, centers = 4, nstart = 5)
class(kmfit)
str(kmfit)
# Cluster - Number of cluster to which dataset belongs
# centers - k Center(finding k cluster)
# totss - The sum of total of squre(Distance)
# withinss - Vector of within-cluster sum of squares, one component per cluster.
# tot.withinss(distortion) - Total within-cluster sum of squares, i.e. sum(withinss). The sum of all withinss
# betweenss - The between-cluster sum of squares, i.e. totss-tot.withinss.
# Size - The number of points in each cluster.
# iter - Number of iteration to converge the cluster
# ifault - integer: indicator of a possible algorithm problem â€“ for experts.
kmfit$centers
kmfit$size
#Plots every combination of two dimensions
plot(tastes)
plot(kmfit)
kmfit$cluster==3
D[kmfit$cluster==3,]
kmfit2 = kmeans(tastes, centers = 4, nstart = 10)
kmfit2$size
#Add another column to store cluster information
D$cluster = kmfit$cluster
str(D)
write.csv(D,"whiskies_post_analsysis.csv")
kmeans.wss.k <- function(D, k){
km <- kmeans(D, centers = k, nstart = 5)
return (km$tot.withinss)
}
D <- read.table("whiskies.txt",header = T, sep = ",")
#Data is from Scotland Whisky
class(D)
str(D)
#location and whisky taste
tastes = D[,3:14]
str(tastes)
#Devide the d
plot(x^2 + 2^x + 1)
plot(log)
plot(ln)
plot(exp)
plot(sin)
plot(cos)
plot(tan)
library(twitterR)
library("twitteR", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library(tm)
install.packages("tm")
install.packages("tm")
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
detach("package:NLP", unload=TRUE)
library("NLP", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library(NLP)
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
api_key <- "x3JPfnDiFx61mURPDlKPKzPe8"
api_secret <- "xbo1ZDSGcHP4qYp24nK4slgFsxvml5TYLQyMOZy1ciTw5uQn5a"
access_token <- "115878452-h21LayhtJbXd4BS0y9G3316zdM5J09dDkpfvNQxq"
access_token_secret <- "fsvLh86IRFUEKhGUKZgDmyV3fJWtP4vfbSI9O8RlmcHri"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
rdmTweets = userTimeline("rdatamining", n=100)
rdmTweets
yTweets = userTimeline("Yahoo", n=500)
tTweets
yTweets
rdmTweets
tweets = fdaTweets
fdaTweets
fdaTweets = userTimeline("FDArecalls", n=500)
fdaTweets
tweets = fdaTweets
search()
str(tweets)
df <- do.call("rbind", lapply(tweets, as.data.frame))
str(df)
df$text
myCorpus = Corpus(VectorSource(df$text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myStopwords <- c(stopwords('english'), "available", "via", "obama", "modi", "sonia", "loksabha")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
inspect(myCorpus[1:3])
myDtm <- DocumentTermMatrix(myCorpus)
dim(myDtm)
myDtm <- DocumentTermMatrix(myCorpus)
dictCorpus <- myCorpus
df$text[1]
inspect(myCorpus[1])
myDtm <- DocumentTermMatrix(myCorpus)
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
dictCorpus <- myCorpus
myDtm <- DocumentTermMatrix(myCorpus)
DB <- read.csv("Diabetes.csv", head = T)
class(DB)
head(DB)
str(DB)
nrow(DB)
flags <- sample(1:2, nrow(DB), replace = T, prob = c(0.8, 0.2))
trainset = DB[flags == 1, ]
testset = DB[flags == 2, ]
nrow(trainset)
nrow(testset)
library(rpart)
names(DB)
#formula: class_label ~ independent_variable
dt = rpart(Class.variable ~ . , trainset, maxdepth = 10)
class(dt)
plot(dt)
text(dt)
summary(dt)
library(rpart.plot)
rpart.plot(dt)
actualLabels = testset$Diabet
actualLabels
names(testset)
testset
####Predict####
dt_predictions = predict(dt, testset, type = "class")
dt_predictions
cbind(as.character(testset$Class.variable),as.character(dt_predictions))
table(testset$Class.variable,dt_predictions )
setwd("~/R/Module4")
DB <- read.csv("Diabetes.csv", head = T)
class(DB)
head(DB)
str(DB)
nrow(DB)
flags <- sample(1:2, nrow(DB), replace = T, prob = c(0.8, 0.2))
trainset = DB[flags == 1, ]
testset = DB[flags == 2, ]
nrow(trainset)
nrow(testset)
library(rpart)
names(DB)
#formula: class_label ~ independent_variable
dt = rpart(Class.variable ~ . , trainset, maxdepth = 10)
class(dt)
plot(dt)
text(dt)
summary(dt)
library(rpart.plot)
rpart.plot(dt)
actualLabels = testset$Diabet
actualLabels
names(testset)
testset
####Predict####
dt_predictions = predict(dt, testset, type = "class")
dt_predictions
cbind(as.character(testset$Class.variable),as.character(dt_predictions))
table(testset$Class.variable,dt_predictions )
#comparison between actual and prediction
table(actualLabels, dt_predictions)
#accuracy
(81+29)/nrow(testset)
TN = CM[1,1]
TP = CM[2,2]
FP = CM[2,1]
FN = CM[1,2]
TP = 37
TN = 83
FN = 13
FP = 18
accuracy
#####confusion matrix######
table(as.character(testset$Class.variable),as.character(dt_predictions))
rf = randomForesr(Class.variable ~ . , trainset, ntree = 50, mtry =3)
rf = randomForest(Class.variable ~ . , trainset, ntree = 50, mtry =3)
install.packages("randomForest")
library(randomForest)
rf = randomForest(Class.variable ~ . , trainset, ntree = 50, mtry =3)
rf_predictions = predict(rf, testset, type = "class")
table(testset$Class.variable, dt_predictions)
table(testset$Class.variable, rf_predictions)
perf = function(actuals, predictions){
tab = table(actuals, predictions)
TP = tab[2,2]
TN = tab[1,1]
FP = tab[1,2]
FN = tba[2,1]
accuracy = (TP+TN)/(TP+TN+FP+FN)
precision = TP/((TP+FP))
recall = TP / (TP+FN)
print(paste("Accuracy = ", accuracy, ", Precision = ", precision, ", Recall = ", recall))
}
table(testset$Class.variable, dt_predictions)
table(testset$Class.variable, rf_predictions)
perf(testset$Class.variable, dt_predictions)
perf(testset$Class.variable, dt_predictions)
perf(testset$Class.variable, rf_predictions)
perf = function(actuals, predictions){
tab = table(actuals, predictions)
TP = tab[2,2]
TN = tab[1,1]
FP = tab[1,2]
FN = tab[2,1]
accuracy = (TP+TN)/(TP+TN+FP+FN)
precision = TP/((TP+FP))
recall = TP / (TP+FN)
print(paste("Accuracy = ", accuracy, ", Precision = ", precision, ", Recall = ", recall))
}
perf(testset$Class.variable, dt_predictions)
perf(testset$Class.variable, rf_predictions)
